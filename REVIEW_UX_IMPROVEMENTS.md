# LLM Council: Deep UX & Functionality Review

**Date:** 2025-12-26
**Reviewer:** Claude (Opus 4.5)

This document provides a rigorous analysis of the current LLM Council implementation with creative, prioritized recommendations for improving functionality and user journey.

---

## Executive Summary

LLM Council is an innovative 3-stage deliberation system that successfully implements anonymous peer review across multiple LLMs. The core concept is strong, but several critical gaps affect user experience, data integrity, and long-term usability.

**Key Findings:**
- **Broken anonymity principle** in Stage 3 (chairman sees model names)
- **Data loss on reload** (metadata not persisted)
- **No user configuration** (models hardcoded)
- **Silent parsing failures** (rankings may be incomplete without indication)
- **Missing conversation management** (no edit/delete/search)

---

## Part 1: Critical Issues (Must Fix)

### 1.1 Chairman Sees Model Names (Breaks Anonymity)

**Current Behavior:** Stage 2 correctly anonymizes responses as "Response A, B, C, D" to prevent bias. However, Stage 3 passes full model names to the chairman:

```python
# council.py:132-135
stage1_text = "\n\n".join([
    f"Model: {result['model']}\nResponse: {result['response']}"  # â† Exposes names!
    for result in stage1_results
])
```

**Impact:** The chairman (e.g., Gemini) may subconsciously favor responses from models it "knows" are strong, defeating the purpose of anonymous peer review.

**Recommendation:**
- Maintain anonymity in Stage 3 by passing only "Response A", "Response B" etc.
- Include a summary of aggregate rankings WITHOUT model names
- De-anonymize only in the chairman's internal reasoning, not the prompt

```python
# Proposed fix
stage1_text = "\n\n".join([
    f"Response {chr(65 + i)}:\n{result['response']}"
    for i, result in enumerate(stage1_results)
])

# Include rankings summary
rankings_summary = "Based on peer review:\n"
rankings_summary += "\n".join([
    f"- Response {label.split()[-1]}: Average rank {agg['average_rank']}"
    for label, agg in zip(label_to_model.keys(), aggregate_rankings)
])
```

---

### 1.2 Metadata Not Persisted (Data Loss on Reload)

**Current Behavior:** `label_to_model` and `aggregate_rankings` are returned via API but NOT saved to storage:

```python
# storage.py
def add_assistant_message(self, conversation_id, stage1, stage2, stage3):
    # â† No metadata parameter!
```

**Impact:**
- After page reload, Stage 2 shows raw "Response A" text instead of de-anonymized model names
- Aggregate rankings disappear entirely
- Inconsistent experience between fresh and reloaded conversations

**Recommendation:** Extend storage schema to persist metadata:

```python
def add_assistant_message(self, conversation_id, stage1, stage2, stage3, metadata=None):
    message = {
        "role": "assistant",
        "stage1": stage1,
        "stage2": stage2,
        "stage3": stage3,
        "metadata": metadata  # Add this
    }
```

---

### 1.3 Silent Ranking Parse Failures

**Current Behavior:** `parse_ranking_from_text()` uses fallback regex and returns partial results without validation:

```python
# If format violated, falls back to ANY "Response X" found
matches = re.findall(r'Response [A-Z]', ranking_section)
return matches  # Could be [A, C] if B missing - no error raised!
```

**Impact:**
- Aggregate rankings computed from incomplete data
- User sees "4 votes" but maybe only 3 were actually parsed correctly
- No visual indication that parsing may have failed

**Recommendation:**
1. Add validation that all expected responses are present
2. Return a confidence indicator with parsed rankings
3. Show warning in UI when parsing is incomplete

```python
def parse_ranking_from_text(ranking_text: str, expected_count: int) -> dict:
    matches = [...]
    return {
        "ranking": matches,
        "complete": len(matches) == expected_count,
        "missing_count": expected_count - len(matches)
    }
```

**UI Enhancement:**
```jsx
{parsedRanking.missing_count > 0 && (
  <span className="parse-warning">
    âš ï¸ {parsedRanking.missing_count} response(s) not found in ranking
  </span>
)}
```

---

### 1.4 No Error Recovery for Partial Failures

**Current Behavior:** If a model fails in Stage 1, it's silently excluded. If parsing fails in Stage 2, it proceeds with partial data.

**Impact:** Users don't know if they're seeing 3 out of 4 models or all 4.

**Recommendation:**
1. Add failure metadata to API response:
```python
metadata = {
    "label_to_model": label_to_model,
    "aggregate_rankings": aggregate_rankings,
    "stage1_failures": ["model/that-failed"],  # New
    "stage2_parse_issues": ["model/with-bad-format"]  # New
}
```

2. Show non-intrusive warning in UI when models failed

---

## Part 2: Major UX Improvements

### 2.1 User-Configurable Council

**Current State:** Models hardcoded in `backend/config.py`. Users must edit code and restart.

**Recommendation:** Add settings UI with:
- Council model selector (multi-select from available models)
- Chairman model selector
- Preset configurations ("Frontier Models", "Budget Models", "Open Source Only")
- Per-conversation override option

**Implementation:**
1. Add `/api/config` endpoint for getting/setting council config
2. Add settings modal in frontend
3. Store user preferences in localStorage + optional backend persistence
4. Allow conversation-level overrides

**Mockup:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Council Settings                        âœ•   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Council Members (select 3-6):               â”‚
â”‚ â˜‘ GPT-5.1      â˜‘ Claude Sonnet 4.5         â”‚
â”‚ â˜‘ Gemini 3 Pro â˜‘ Grok 4                    â”‚
â”‚ â˜ Llama 3.1   â˜ Mixtral                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chairman: [Gemini 3 Pro         â–¾]          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Presets: [Frontier] [Budget] [Open Source]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.2 Conversation Management

**Current State:** No delete, no rename, no search, no export.

**Recommendations:**

**A) Delete Conversations:**
- Add trash icon on hover in sidebar
- Confirm dialog before deletion
- Soft delete with 30-day recovery option

**B) Rename Conversations:**
- Double-click title to edit inline
- Or context menu â†’ "Rename"

**C) Search Across Conversations:**
- Add search box at top of sidebar
- Search conversation titles and message content
- Highlight matching conversations

**D) Export Options:**
- Export single conversation as Markdown
- Export as JSON (for data portability)
- "Copy link" for future sharing feature

---

### 2.3 Model Display Names

**Current State:** Full identifiers like "claude-sonnet-4.5" shown in tabs.

**Problem:** Long names, inconsistent capitalization, not user-friendly.

**Recommendation:** Add display name mapping:

```javascript
const MODEL_DISPLAY_NAMES = {
  "openai/gpt-5.1": "GPT-5.1",
  "anthropic/claude-sonnet-4.5": "Claude 4.5",
  "google/gemini-3-pro-preview": "Gemini 3",
  "x-ai/grok-4": "Grok 4",
};

function getDisplayName(modelId) {
  return MODEL_DISPLAY_NAMES[modelId] || modelId.split('/')[1];
}
```

Also add provider icons/colors for quick visual identification.

---

### 2.4 Improved Progress Indication

**Current State:** Three numbered circles with pending/active/completed states.

**Enhancement Ideas:**

**A) Time Estimates:**
- Track average response time per model
- Show estimated remaining time
- "Stage 1: ~15 seconds remaining"

**B) Per-Model Progress:**
- During Stage 1, show which models have responded
- "2/4 models responded" with checkmarks

**C) Streaming Token Counts:**
- Show tokens received in real-time
- Gives users something to watch during long waits

**Mockup:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: Individual Responses              â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 3/4 models          â”‚
â”‚                                            â”‚
â”‚ âœ“ GPT-5.1 (1,247 tokens)                  â”‚
â”‚ âœ“ Claude 4.5 (892 tokens)                 â”‚
â”‚ âœ“ Grok 4 (1,105 tokens)                   â”‚
â”‚ â³ Gemini 3 (receiving...)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.5 Side-by-Side Response Comparison

**Current State:** Tab interface - can only view one response at a time.

**Recommendation:** Add comparison view:
- 2-panel side-by-side layout
- Dropdown to select which 2 responses to compare
- Optional diff highlighting for similar/different sections

**Mockup:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GPT-5.1        â–¾    â”‚ Claude 4.5     â–¾    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ The answer is...    â”‚ The answer is...    â”‚
â”‚                     â”‚                     â”‚
â”‚ First, consider     â”‚ Initially, we must  â”‚
â”‚ [highlighted diff]  â”‚ [highlighted diff]  â”‚
â”‚                     â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.6 Keyboard Navigation

**Current State:** Only Enter/Shift+Enter for sending messages.

**Recommendation:** Add comprehensive shortcuts:

| Shortcut | Action |
|----------|--------|
| `Cmd/Ctrl + N` | New conversation |
| `Cmd/Ctrl + 1-3` | Jump to Stage 1/2/3 |
| `Tab` | Cycle through response tabs |
| `Cmd/Ctrl + C` (with selection) | Copy selected text |
| `Cmd/Ctrl + Shift + C` | Copy entire response |
| `Esc` | Close modals / cancel |
| `â†‘/â†“` (in sidebar) | Navigate conversations |

Add keyboard shortcut help modal (`?` key).

---

## Part 3: Creative Feature Ideas

### 3.1 "Reasoning Reveal" Mode

**Concept:** Some models (o1, Claude with extended thinking) provide chain-of-thought reasoning. Currently this is captured but not displayed.

**Implementation:**
- Add expandable "Show Reasoning" section under each response
- Collapsible by default to avoid clutter
- Different styling (monospace, muted colors) to distinguish from answer

```jsx
{response.reasoning_details && (
  <details className="reasoning-section">
    <summary>ğŸ’­ Show reasoning ({response.reasoning_details.length} chars)</summary>
    <pre>{response.reasoning_details}</pre>
  </details>
)}
```

---

### 3.2 "Model Hall of Fame"

**Concept:** Track aggregate rankings over time across all conversations.

**Implementation:**
- Store ranking results in a separate analytics table
- Dashboard view showing:
  - Total wins per model
  - Average rank per model over time
  - Performance by question category
  - Head-to-head records

**Mockup:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Hall of Fame (Last 30 Days)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ¥‡ Claude 4.5    Avg: 1.4  â”‚ Wins: 47      â”‚
â”‚ ğŸ¥ˆ GPT-5.1       Avg: 1.8  â”‚ Wins: 32      â”‚
â”‚ ğŸ¥‰ Gemini 3      Avg: 2.1  â”‚ Wins: 28      â”‚
â”‚    Grok 4        Avg: 2.7  â”‚ Wins: 13      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [View by Category â–¾] [Time Range â–¾]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3.3 "Council Debate" Mode

**Concept:** Instead of one-shot responses, models can see and respond to each other's answers in a multi-round debate format.

**Flow:**
1. Stage 1: Initial responses (same as now)
2. **New Stage 1.5:** Each model sees others' responses and provides a rebuttal/refinement
3. Stage 2: Ranking includes both initial and refined responses
4. Stage 3: Chairman synthesizes the full debate

**Benefits:**
- Models can correct each other's mistakes
- Surfaces disagreements more clearly
- More engaging for complex topics

---

### 3.4 "Question Decomposition" Mode

**Concept:** For complex questions, automatically break them into sub-questions, run council on each, then synthesize.

**Example:**
```
User: "Should I use React or Vue for my startup's dashboard?"

System decomposes:
1. "What are React's strengths for dashboards?"
2. "What are Vue's strengths for dashboards?"
3. "What factors matter most for startups?"
4. "How do React and Vue compare on learning curve?"

â†’ Run council on each sub-question
â†’ Synthesize into comprehensive answer
```

---

### 3.5 "Citation Mode"

**Concept:** For factual questions, require models to cite sources and validate claims.

**Implementation:**
- Modify Stage 1 prompt to require citations
- Stage 2 evaluators check citation validity
- Chairman synthesizes with verified citations
- UI shows citation links and verification status

---

### 3.6 Custom Evaluation Criteria

**Current State:** Models evaluate based on generic "accuracy and insight."

**Recommendation:** Let users specify evaluation criteria:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Evaluation Criteria:                        â”‚
â”‚                                             â”‚
â”‚ â˜‘ Accuracy        â˜‘ Clarity                â”‚
â”‚ â˜‘ Completeness    â˜ Brevity                â”‚
â”‚ â˜ Creativity      â˜‘ Practical utility      â”‚
â”‚                                             â”‚
â”‚ Custom: [Be especially critical of___    ]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Inject criteria into Stage 2 prompt:
```
Evaluate responses based on:
- Accuracy (40% weight)
- Clarity (30% weight)
- Practical utility (30% weight)
```

---

### 3.7 "Confidence Calibration"

**Concept:** Ask models to rate their confidence, then compare confidence to peer rankings.

**Implementation:**
- Stage 1 prompt asks: "Rate your confidence 1-10"
- Stage 2 asks evaluators to rate other responses' confidence
- Display confidence vs actual rank scatter plot
- Identify which models are well-calibrated vs overconfident

---

### 3.8 Follow-up Question Suggestions

**Concept:** After Stage 3, suggest follow-up questions based on the discussion.

**Implementation:**
- Add to chairman prompt: "Suggest 3 follow-up questions the user might ask"
- Display as clickable chips below Stage 3
- Clicking auto-fills the input

```jsx
<div className="follow-up-suggestions">
  <span>Follow up:</span>
  <button onClick={() => setInput("How does this compare to...")}>
    Compare to alternatives?
  </button>
  <button onClick={() => setInput("Can you provide examples...")}>
    Give examples?
  </button>
</div>
```

---

## Part 4: Accessibility & Polish

### 4.1 Accessibility Improvements

**A) ARIA Labels:**
```jsx
<button
  aria-label="Copy response to clipboard"
  aria-pressed={copied}
>
```

**B) Focus Management:**
- Focus input after sending message
- Focus first tab when stage loads
- Trap focus in modals

**C) Color Independence:**
- Add icons alongside colors for stages
- Stage 1: ğŸ“‹ or list icon
- Stage 2: âš–ï¸ or scale icon
- Stage 3: âœ“ or checkmark icon

**D) Screen Reader Announcements:**
```jsx
<div role="status" aria-live="polite">
  Stage 1 complete. Stage 2 starting.
</div>
```

---

### 4.2 Dark Mode

**Implementation approach:**
1. CSS custom properties for all colors
2. Toggle in header
3. Save preference to localStorage
4. System preference detection

```css
:root {
  --bg-primary: #ffffff;
  --text-primary: #333333;
  --accent: #4a90e2;
}

[data-theme="dark"] {
  --bg-primary: #1a1a2e;
  --text-primary: #e0e0e0;
  --accent: #64b5f6;
}
```

---

### 4.3 Responsive Design

**Current State:** Fixed sidebar width, no mobile consideration.

**Recommendations:**
- Collapsible sidebar on mobile
- Full-width chat on small screens
- Swipeable tabs for responses
- Floating action button for new conversation

---

### 4.4 Loading Skeletons

Replace spinners with content-aware skeletons:

```jsx
// Instead of just spinner
<div className="skeleton-response">
  <div className="skeleton-line" style={{width: '80%'}}></div>
  <div className="skeleton-line" style={{width: '95%'}}></div>
  <div className="skeleton-line" style={{width: '70%'}}></div>
</div>
```

---

## Part 5: Technical Debt & Backend Improvements

### 5.1 Environment-Based Configuration

**Current State:** Hardcoded `API_BASE = 'http://localhost:8001'`

**Recommendation:**
```javascript
// api.js
const API_BASE = import.meta.env.VITE_API_URL || 'http://localhost:8001';
```

```python
# config.py
import os
PORT = int(os.getenv("PORT", 8001))
```

---

### 5.2 Rate Limiting

**Current State:** No limits - can spam expensive API calls.

**Recommendation:**
- Add per-IP rate limiting (e.g., 10 requests/minute)
- Display rate limit status in UI
- Queue requests when approaching limit

```python
from slowapi import Limiter
limiter = Limiter(key_func=get_remote_address)

@app.post("/api/conversations/{id}/message")
@limiter.limit("10/minute")
async def send_message(...):
```

---

### 5.3 Logging & Observability

**Current State:** Errors print to console only.

**Recommendation:**
- Structured logging with request IDs
- Log model latencies for performance tracking
- Error aggregation service integration

```python
import structlog
logger = structlog.get_logger()

logger.info("stage1_complete",
    models_succeeded=len(results),
    models_failed=len(failures),
    duration_ms=elapsed)
```

---

### 5.4 Input Validation

**Current State:** User queries passed directly to models.

**Recommendation:**
- Max query length (e.g., 10,000 chars)
- Content filtering for obvious attacks
- Sanitize before storage

```python
from pydantic import Field, validator

class MessageRequest(BaseModel):
    content: str = Field(..., min_length=1, max_length=10000)

    @validator('content')
    def sanitize_content(cls, v):
        # Basic sanitization
        return v.strip()
```

---

### 5.5 API Versioning

**Recommendation:** Version the API for future breaking changes:

```python
app = FastAPI()
v1_router = APIRouter(prefix="/api/v1")

# Old routes still work
@app.get("/api/conversations")  # deprecated
# New versioned routes
@v1_router.get("/conversations")
```

---

## Implementation Priority Matrix

| Priority | Issue | Effort | Impact |
|----------|-------|--------|--------|
| **P0 - Critical** | | | |
| | Persist metadata | Low | High |
| | Fix chairman anonymity | Medium | High |
| | Add parse validation | Low | Medium |
| **P1 - High** | | | |
| | Model configuration UI | High | High |
| | Conversation delete | Low | Medium |
| | Display name mapping | Low | Medium |
| | Dark mode | Medium | Medium |
| **P2 - Medium** | | | |
| | Conversation search | Medium | Medium |
| | Side-by-side comparison | Medium | Medium |
| | Keyboard shortcuts | Low | Low |
| | Loading skeletons | Low | Low |
| **P3 - Nice to Have** | | | |
| | Model Hall of Fame | High | Medium |
| | Debate mode | High | Medium |
| | Reasoning reveal | Low | Low |
| | Follow-up suggestions | Medium | Low |

---

## Conclusion

LLM Council has a solid foundation with an innovative approach to multi-LLM deliberation. The core 3-stage flow is well-implemented with proper streaming and progress indication.

**Top 5 Immediate Wins:**
1. **Persist metadata** - Fixes reload inconsistency with minimal code
2. **Fix chairman anonymity** - Restores integrity of the experiment
3. **Add model display names** - Quick UI polish with big readability win
4. **Add conversation delete** - Basic housekeeping users expect
5. **Add parse validation warnings** - Surfaces hidden data quality issues

**Top 3 High-Impact Features:**
1. **User-configurable council** - Transforms from demo to useful tool
2. **Model Hall of Fame** - Creates compelling ongoing value
3. **Side-by-side comparison** - Dramatically improves response comparison UX

The project is in excellent position to evolve from a "vibe-coded weekend hack" into a genuinely useful LLM comparison and deliberation tool.
